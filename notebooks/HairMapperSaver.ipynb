{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b444acd0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b444acd0",
        "outputId": "0c239593-31d0-4b2c-9a03-9593ea72dbc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:pydrive is deprecated and no longer maintained. We recommend that you migrate your projects to pydrive2, the maintained fork of pydrive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Репозиторий уже существует: ./HairMapper\n",
            "[INFO] Текущая рабочая директория: /content/HairMapper\n",
            "[INFO] Файл уже существует: ./ckpts/StyleGAN2-ada-Generator.pth\n",
            "[INFO] Файл уже существует: ./ckpts/e4e_ffhq_encode.pt\n",
            "[INFO] Файл уже существует: ./ckpts/model_ir_se50.pth\n",
            "[INFO] Файл уже существует: ./ckpts/face_parsing.pth\n",
            "[INFO] Файл уже существует: ./ckpts/vgg16.pth\n",
            "[INFO] Файл уже существует: ./classifier/gender_classification/classification_model.pth\n",
            "[INFO] Файл уже существует: ./classifier/hair_classification/classification_model.pth\n"
          ]
        }
      ],
      "source": [
        "# Ячейка 1: Setup и загрузка моделей\n",
        "import os\n",
        "import subprocess\n",
        "import glob\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth  # Только для Colab\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Функция клонирования репозитория, если папка ещё не существует\n",
        "def clone_repo_if_not_exists(repo_url, dest_folder):\n",
        "    if os.path.exists(dest_folder):\n",
        "        print(f\"[INFO] Репозиторий уже существует: {dest_folder}\")\n",
        "        return\n",
        "    print(f\"[INFO] Клонирование репозитория: {repo_url}\")\n",
        "    subprocess.run(['git', 'clone', repo_url, dest_folder], check=True)\n",
        "    print(f\"[INFO] Репозиторий успешно клонирован в: {dest_folder}\")\n",
        "\n",
        "# Функция для скачивания файла с Google Drive, если он ещё не скачан\n",
        "def download_from_google_drive(file_id, file_dst):\n",
        "    if os.path.exists(file_dst):\n",
        "        print(f\"[INFO] Файл уже существует: {file_dst}\")\n",
        "        return\n",
        "    print(f\"[INFO] Скачивание файла в: {file_dst}\")\n",
        "    downloaded = drive.CreateFile({'id': file_id})\n",
        "    downloaded.FetchMetadata(fetch_all=True)\n",
        "    downloaded.GetContentFile(file_dst)\n",
        "    print(f\"[INFO] Файл успешно скачан: {file_dst}\")\n",
        "\n",
        "# Аутентификация в Google Drive (требуется в Colab)\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# Клонируем репозиторий HairMapper, если он ещё не существует\n",
        "REPO_URL = \"https://github.com/Lunatik-006/HairMapper.git\"\n",
        "REPO_FOLDER = \"./HairMapper\"\n",
        "clone_repo_if_not_exists(REPO_URL, REPO_FOLDER)\n",
        "\n",
        "# Переходим в папку репозитория\n",
        "os.chdir(REPO_FOLDER)\n",
        "print(\"[INFO] Текущая рабочая директория:\", os.getcwd())\n",
        "\n",
        "# Загрузка чекпоинтов моделей\n",
        "checkpoints = {\n",
        "    'StyleGAN2-ada-Generator.pth': {'url': '1EsGehuEdY4z4t21o2LgW2dSsyN3rxYLJ', 'dir': './ckpts'},\n",
        "    'e4e_ffhq_encode.pt': {'url': '1cUv_reLE6k3604or78EranS7XzuVMWeO', 'dir': './ckpts'},\n",
        "    'model_ir_se50.pth': {'url': '1GIMopzrt2GE_4PG-_YxmVqTQEiaqu5L6', 'dir': './ckpts'},\n",
        "    'face_parsing.pth': {'url': '1IMsrkXA9NuCEy1ij8c8o6wCrAxkmjNPZ', 'dir': './ckpts'},\n",
        "    'vgg16.pth': {'url': '1EPhkEP_1O7ZVk66aBeKoFqf3xiM4BHH8', 'dir': './ckpts'}\n",
        "}\n",
        "for ckpt_name, info in checkpoints.items():\n",
        "    output_dir = info['dir']\n",
        "    os.makedirs(output_dir, exist_ok=True)  # Создаем папку, если её нет\n",
        "    output_path = os.path.join(output_dir, ckpt_name)\n",
        "    download_from_google_drive(file_id=info['url'], file_dst=output_path)\n",
        "\n",
        "# Загрузка чекпоинтов классификаторов (gender/hair)\n",
        "classification_ckpt = [\n",
        "    {'url': '1SSw6vd-25OGnLAE0kuA-_VHabxlsdLXL', 'dir': './classifier/gender_classification'},\n",
        "    {'url': '1n14ckDcgiy7eu-e9XZhqQYb5025PjSpV', 'dir': './classifier/hair_classification'}\n",
        "]\n",
        "for clf in classification_ckpt:\n",
        "    output_dir = clf['dir']\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    output_path = os.path.join(output_dir, 'classification_model.pth')\n",
        "    download_from_google_drive(file_id=clf['url'], file_dst=output_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "86004b72",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86004b72",
        "outputId": "2aba1163-00ef-4851-c31f-3e0e07abb89e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Requirement already satisfied: torch===2.0.0+cu117 in /usr/local/lib/python3.11/dist-packages (2.0.0+cu117)\n",
            "Requirement already satisfied: torchvision===0.15.0+cu117 in /usr/local/lib/python3.11/dist-packages (0.15.0+cu117)\n",
            "Requirement already satisfied: torchaudio===2.0.0+cu117 in /usr/local/lib/python3.11/dist-packages (2.0.0+cu117)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch===2.0.0+cu117) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch===2.0.0+cu117) (4.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch===2.0.0+cu117) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch===2.0.0+cu117) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch===2.0.0+cu117) (3.1.6)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.11/dist-packages (from torch===2.0.0+cu117) (2.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision===0.15.0+cu117) (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchvision===0.15.0+cu117) (2.32.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision===0.15.0+cu117) (11.1.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch===2.0.0+cu117) (3.31.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch===2.0.0+cu117) (18.1.8)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch===2.0.0+cu117) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision===0.15.0+cu117) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision===0.15.0+cu117) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision===0.15.0+cu117) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision===0.15.0+cu117) (2025.1.31)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch===2.0.0+cu117) (1.3.0)\n",
            "Collecting tqdm==4.60.0 (from -r requirements.txt (line 1))\n",
            "  Using cached tqdm-4.60.0-py2.py3-none-any.whl.metadata (57 kB)\n",
            "Collecting requests==2.25.1 (from -r requirements.txt (line 2))\n",
            "  Using cached requests-2.25.1-py2.py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting matplotlib==3.4.1 (from -r requirements.txt (line 3))\n",
            "  Using cached matplotlib-3.4.1.tar.gz (37.3 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting PyYAML==5.4.1 (from -r requirements.txt (line 4))\n",
            "  Using cached PyYAML-5.4.1.tar.gz (175 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "Requirement already satisfied: numpy<2.0 in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "--2025-04-07 23:01:54--  https://github.com/ninja-build/ninja/releases/download/v1.8.2/ninja-linux.zip\n",
            "Resolving github.com (github.com)... 20.27.177.113\n",
            "Connecting to github.com (github.com)|20.27.177.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/1335132/d2f252e2-9801-11e7-9fbf-bc7b4e4b5c83?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250407%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250407T230154Z&X-Amz-Expires=300&X-Amz-Signature=f29ee3141fd3611c14bc8e1b36faf03d75fc8c38d5cb65ebfb9971b4d63e5cb8&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dninja-linux.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-04-07 23:01:54--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/1335132/d2f252e2-9801-11e7-9fbf-bc7b4e4b5c83?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250407%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250407T230154Z&X-Amz-Expires=300&X-Amz-Signature=f29ee3141fd3611c14bc8e1b36faf03d75fc8c38d5cb65ebfb9971b4d63e5cb8&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dninja-linux.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 77854 (76K) [application/octet-stream]\n",
            "Saving to: ‘ninja-linux.zip.2’\n",
            "\n",
            "ninja-linux.zip.2   100%[===================>]  76.03K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-04-07 23:01:55 (763 KB/s) - ‘ninja-linux.zip.2’ saved [77854/77854]\n",
            "\n",
            "Archive:  ninja-linux.zip\n",
            "replace /usr/local/bin/ninja? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: /usr/local/bin/ninja    \n"
          ]
        }
      ],
      "source": [
        "# Ячейка 2: Установка пакетов\n",
        "!pip install torch===2.0.0+cu117 torchvision===0.15.0+cu117 torchaudio===2.0.0+cu117 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install -r requirements.txt\n",
        "!pip install \"numpy<2.0\"\n",
        "!wget https://github.com/ninja-build/ninja/releases/download/v1.8.2/ninja-linux.zip\n",
        "!sudo unzip ninja-linux.zip -d /usr/local/bin/\n",
        "!sudo update-alternatives --install /usr/bin/ninja ninja /usr/local/bin/ninja 1 --force\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from argparse import Namespace\n",
        "import sys\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "from PIL import ImageFile\n",
        "import glob\n",
        "import os\n",
        "import argparse\n",
        "\n",
        "\n",
        "# Здесь загружается модель mapper из папки ./mapper/checkpoints/final\n",
        "mapper_url = 'https://drive.google.com/file/d/1F3oujXbvalqEOixcAkIyURuY512nmroe'\n",
        "mapper_id = mapper_url.replace('https://drive.google.com/file/d/', '').split('/')[0]\n",
        "mapper_output_dir = './mapper/checkpoints/final'\n",
        "os.makedirs(mapper_output_dir, exist_ok=True)\n",
        "mapper_output_path = os.path.join(mapper_output_dir, 'best_model.pt')\n",
        "download_from_google_drive(file_id=mapper_id, file_dst=mapper_output_path)\n",
        "\n",
        "# Импорт необходимых классов для mapper и генератора\n",
        "# (Убедитесь, что пути импорта соответствуют структуре репозитория HairMapper)\n",
        "from mapper.networks.level_mapper import LevelMapper\n",
        "from styleGAN2_ada_model.stylegan2_ada_generator import StyleGAN2adaGenerator\n",
        "from classifier.src.feature_extractor.hair_mask_extractor import get_hair_mask, get_parsingNet\n",
        "\n",
        "# Инициализация генератора (StyleGAN2-ada)\n",
        "model_name = 'stylegan2_ada'\n",
        "print(\"[INFO] Инициализация генератора.\")\n",
        "model = StyleGAN2adaGenerator(model_name, logger=None, truncation_psi=1.0)\n",
        "\n",
        "# Инициализация mapper\n",
        "mapper = LevelMapper(input_dim=512).eval().cuda()\n",
        "mapper_ckpt = torch.load(mapper_output_path, map_location='cpu')\n",
        "alpha = float(mapper_ckpt['alpha']) * 1.2\n",
        "mapper.load_state_dict(mapper_ckpt['state_dict'], strict=True)\n",
        "\n",
        "# Параметры для генератора\n",
        "latent_space_type = 'wp'\n",
        "kwargs = {'latent_space_type': latent_space_type}\n",
        "\n",
        "# Загрузка парсинговой сети для извлечения маски волос\n",
        "parsingNet = get_parsingNet(save_pth='./HairMapper/ckpts/face_parsing.pth')\n",
        "\n",
        "# Тестовое изображение загружается в папку ./test_data/origin (для отладки)\n",
        "test_img_name = 'test_img.png'\n",
        "test_img_id = '1Ju5jLtNCALHJ2crJkMr00UP_ZUQzQBRs'\n",
        "test_img_dir = './test_data/origin'\n",
        "os.makedirs(test_img_dir, exist_ok=True)\n",
        "test_img_path = os.path.join(test_img_dir, test_img_name)\n",
        "download_from_google_drive(file_id=test_img_id, file_dst=test_img_path)\n"
      ],
      "metadata": {
        "id": "qqGi4R7gpdno",
        "outputId": "86499b49-fccb-46ca-fd01-98f28ae1bdb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        }
      },
      "id": "qqGi4R7gpdno",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:stylegan2_ada_generator:Load model from /content/HairMapper/ckpts/StyleGAN2-ada-Generator.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Файл уже существует: ./mapper/checkpoints/final/best_model.pt\n",
            "[INFO] Инициализация генератора.\n",
            "Loading pytorch model from `/content/HairMapper/ckpts/StyleGAN2-ada-Generator.pth`.\n",
            "load face_parsing model from:  ./HairMapper/ckpts/face_parsing.pth\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './HairMapper/ckpts/face_parsing.pth'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-4704828a9342>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# Загрузка парсинговой сети для извлечения маски волос\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mparsingNet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_parsingNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_pth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./HairMapper/ckpts/face_parsing.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# Тестовое изображение загружается в папку ./test_data/origin (для отладки)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/HairMapper/classifier/src/feature_extractor/hair_mask_extractor.py\u001b[0m in \u001b[0;36mget_parsingNet\u001b[0;34m(save_pth)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'load face_parsing model from: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_pth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m     \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_pth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m     \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    792\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './HairMapper/ckpts/face_parsing.pth'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c21a007c",
      "metadata": {
        "id": "c21a007c"
      },
      "outputs": [],
      "source": [
        "# Ячейка 4: Переход в папку encoder4editing и загрузка модели pSp (энкодер e4e)\n",
        "os.chdir('./HairMapper/encoder4editing')\n",
        "print(\"[INFO] Текущая директория (encoder4editing):\", os.getcwd())\n",
        "\n",
        "import PIL._util\n",
        "if not hasattr(PIL._util, 'is_directory'):\n",
        "    import os\n",
        "    PIL._util.is_directory = lambda path: os.path.isdir(path)\n",
        "\n",
        "\n",
        "from argparse import Namespace\n",
        "import sys\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "from PIL import ImageFile\n",
        "import glob\n",
        "import argparse\n",
        "\n",
        "sys.path.append(\".\")\n",
        "sys.path.append(\"..\")\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "from models.psp import pSp  # Импорт модели pSp\n",
        "\n",
        "# Трансформации для входного изображения (из папки с исходными изображениями)\n",
        "img_transforms = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5,0.5,0.5], [0.5,0.5,0.5])\n",
        "])\n",
        "\n",
        "# Загрузка чекпоинта энкодера e4e из папки ./ckpts\n",
        "model_path = \"../ckpts/e4e_ffhq_encode.pt\"\n",
        "ckpt = torch.load(model_path, map_location='cpu')\n",
        "opts = ckpt['opts']\n",
        "opts['checkpoint_path'] = model_path\n",
        "opts = Namespace(**opts)\n",
        "net = pSp(opts)\n",
        "net.eval()\n",
        "net.cuda()\n",
        "print(\"[INFO] Модель pSp (энкодер e4e) загружена.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#############################\n",
        "# Ячейка 5: Обработка изображений из папки на Google Drive с входными данными\n",
        "#############################\n",
        "import zipfile\n",
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# Укажите ID входной папки на Google Drive с исходными изображениями\n",
        "# (например, ссылка вида https://drive.google.com/drive/folders/XXX, где XXX - это ID)\n",
        "input_folder_id = '15QuG_Iu8JAmVOJ-9HEJzBrk80NujHbqb'  # Замените на реальный ID входной папки\n",
        "\n",
        "# Локальные папки для временного хранения входных и выходных изображений\n",
        "temp_input_folder = './temp_input'\n",
        "os.makedirs(temp_input_folder, exist_ok=True)\n",
        "output_folder = './temp_output'\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# Получение списка файлов из входной папки по MIME-типу (png и jpg)\n",
        "query = f\"'{input_folder_id}' in parents and (mimeType='image/png')\"\n",
        "input_file_list = drive.ListFile({'q': query}).GetList()\n",
        "total_files = len(input_file_list)\n",
        "print(f\"[INFO] Найдено {total_files} входных изображений.\")\n",
        "\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "\n",
        "pbar = tqdm(total=total_files)\n",
        "for f in input_file_list:\n",
        "    pbar.update(1)\n",
        "    file_title = f['title']\n",
        "    # Скачиваем файл во временную папку\n",
        "    local_input_path = os.path.join(temp_input_folder, file_title)\n",
        "    f.GetContentFile(local_input_path)\n",
        "\n",
        "    # Открываем изображение с помощью PIL и применяем трансформации\n",
        "    try:\n",
        "        input_img = PIL.Image.open(local_input_path).convert('RGB')\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] Не удалось открыть {file_title}: {e}\")\n",
        "        continue\n",
        "    transformed_image = img_transforms(input_img)\n",
        "\n",
        "    # Получаем латентное представление с помощью модели pSp\n",
        "    with torch.no_grad():\n",
        "        latents = net(transformed_image.unsqueeze(0).cuda().float(), randomize_noise=False, return_latents=True)\n",
        "        latent = latents[0].cpu().numpy()\n",
        "        latent = np.reshape(latent, (1, 18, 512))\n",
        "\n",
        "    # Применяем mapper для корректировки латентного кода (удаление волос)\n",
        "    mapper_input = latent.copy()\n",
        "    mapper_input_tensor = torch.from_numpy(mapper_input).cuda().float()\n",
        "    edited_latent_codes = latent.copy()\n",
        "    edited_latent_codes[:, :8, :] += alpha * mapper(mapper_input_tensor).to('cpu').detach().numpy()\n",
        "\n",
        "    # Генерация нового изображения с помощью генератора StyleGAN2-ada\n",
        "    outputs = model.easy_style_mixing(latent_codes=edited_latent_codes,\n",
        "                                      style_range=range(7, 18),\n",
        "                                      style_codes=latent,\n",
        "                                      mix_ratio=0.8,\n",
        "                                      **kwargs)\n",
        "    edited_img = outputs['image'][0][:, :, ::-1]  # Преобразование из BGR в RGB\n",
        "\n",
        "    # Получаем исходное изображение для получения маски (считываем локально скачанный файл)\n",
        "    origin_img = cv2.imread(local_input_path)\n",
        "\n",
        "    # Извлекаем маску волос и применяем seamlessClone\n",
        "    hair_mask = get_hair_mask(img_path=origin_img, net=parsingNet, include_hat=True, include_ear=True)\n",
        "    mask_dilate = cv2.dilate(hair_mask, kernel=np.ones((50, 50), np.uint8))\n",
        "    mask_dilate_blur = cv2.blur(mask_dilate, ksize=(30, 30))\n",
        "    mask_dilate_blur = (hair_mask + (255 - hair_mask)/255 * mask_dilate_blur).astype(np.uint8)\n",
        "    face_mask = 255 - mask_dilate_blur\n",
        "    face_mask = cv2.resize(face_mask, (origin_img.shape[1], origin_img.shape[0]))\n",
        "    idx = np.where(face_mask > 0)\n",
        "    cy = (np.min(idx[0]) + np.max(idx[0])) // 2\n",
        "    cx = (np.min(idx[1]) + np.max(idx[1])) // 2\n",
        "    center = (cx, cy)\n",
        "\n",
        "    # Применяем seamlessClone для объединения изображений\n",
        "    result_img = cv2.seamlessClone(origin_img, edited_img, face_mask[:, :, 0], center, cv2.NORMAL_CLONE)\n",
        "\n",
        "    # Сохраняем обработанное изображение в папку output_folder с расширением .png\n",
        "    output_filename = os.path.splitext(file_title)[0] + '.png'\n",
        "    local_output_path = os.path.join(output_folder, output_filename)\n",
        "    cv2.imwrite(local_output_path, result_img)\n",
        "pbar.close()\n",
        "\n",
        "print(\"[INFO] Обработка завершена. Обработанные изображения сохранены в:\", output_folder)\n",
        "\n",
        "# Создаем ZIP-архив из папки с обработанными изображениями\n",
        "zip_filename = \"processed_images.zip\"\n",
        "!zip -r {zip_filename} {output_folder}\n",
        "print(f\"[INFO] Архив {zip_filename} создан.\")\n",
        "\n",
        "# Автоматическое скачивание ZIP-архива на ПК\n",
        "files.download(zip_filename)"
      ],
      "metadata": {
        "id": "2LDeS1QzppjW"
      },
      "id": "2LDeS1QzppjW",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}